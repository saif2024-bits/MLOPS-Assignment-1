{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Heart Disease UCI Dataset\n",
    "## MLOps Assignment - Task 1\n",
    "\n",
    "**Objective:** Perform comprehensive EDA on the Heart Disease dataset to understand:\n",
    "- Data quality and missing values\n",
    "- Feature distributions\n",
    "- Correlations between features\n",
    "- Class balance\n",
    "- Insights for feature engineering\n",
    "\n",
    "**Dataset:** Heart Disease UCI Dataset (303 samples, 13 features + 1 target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/heart_disease.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*70)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*70)\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(\"Missing Values Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "missing_data = missing_df[missing_df['Missing_Count'] > 0]\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    ax.barh(missing_data.index, missing_data['Percentage'], color='coral')\n",
    "    ax.set_xlabel('Percentage of Missing Values', fontsize=12)\n",
    "    ax.set_title('Missing Values by Feature', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(missing_data['Percentage']):\n",
    "        ax.text(v + 0.1, i, f'{v:.2f}%', va='center')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No Missing Values Found!', \n",
    "            ha='center', va='center', fontsize=16, transform=ax.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/01_missing_values.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "target_counts = df['target'].value_counts()\n",
    "target_percentages = df['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Class 0 (No Disease): {target_counts[0]} ({target_percentages[0]:.2f}%)\")\n",
    "print(f\"Class 1 (Disease): {target_counts[1]} ({target_percentages[1]:.2f}%)\")\n",
    "print(f\"\\nClass Balance Ratio: {target_counts[1]/target_counts[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class balance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "colors = ['#66c2a5', '#fc8d62']\n",
    "target_counts.plot(kind='bar', ax=axes[0], color=colors, edgecolor='black')\n",
    "axes[0].set_title('Class Distribution - Count', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Target Class', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['No Disease (0)', 'Disease (1)'], rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(target_counts):\n",
    "    axes[0].text(i, v + 3, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(target_counts, labels=['No Disease', 'Disease'], autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, explode=(0.05, 0.05))\n",
    "axes[1].set_title('Class Distribution - Percentage', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/02_class_balance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical features\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "\n",
    "print(f\"Numerical Features: {numerical_features}\")\n",
    "print(f\"Categorical Features: {categorical_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    # Histogram with KDE\n",
    "    axes[idx].hist(df[feature].dropna(), bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    \n",
    "    # Add KDE\n",
    "    df[feature].dropna().plot(kind='kde', ax=axes[idx], secondary_y=True, color='red', linewidth=2)\n",
    "    \n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_val = df[feature].mean()\n",
    "    median_val = df[feature].median()\n",
    "    axes[idx].axvline(mean_val, color='green', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    axes[idx].axvline(median_val, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/03_numerical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categorical features\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    value_counts = df[feature].value_counts().sort_index()\n",
    "    \n",
    "    axes[idx].bar(value_counts.index, value_counts.values, edgecolor='black', alpha=0.8)\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('Count', fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(value_counts.index[i], v + 2, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/04_categorical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            vmin=-1, vmax=1, ax=ax)\n",
    "\n",
    "ax.set_title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/05_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Top correlations with target\n",
    "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
    "print(\"\\nTop Correlations with Target:\")\n",
    "print(\"=\"*70)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top correlations with target\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "target_corr_sorted = target_corr.drop('target').sort_values()\n",
    "colors_corr = ['red' if x < 0 else 'green' for x in target_corr_sorted]\n",
    "\n",
    "ax.barh(range(len(target_corr_sorted)), target_corr_sorted.values, color=colors_corr, alpha=0.7, edgecolor='black')\n",
    "ax.set_yticks(range(len(target_corr_sorted)))\n",
    "ax.set_yticklabels(target_corr_sorted.index)\n",
    "ax.set_xlabel('Correlation Coefficient', fontsize=12)\n",
    "ax.set_title('Feature Correlation with Target', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(target_corr_sorted.values):\n",
    "    ax.text(v + 0.01 if v > 0 else v - 0.01, i, f'{v:.3f}', \n",
    "            va='center', ha='left' if v > 0 else 'right', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/06_target_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bivariate Analysis - Features vs Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features by target\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(numerical_features):\n",
    "    df.boxplot(column=feature, by='target', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{feature} by Target Class', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Target (0: No Disease, 1: Disease)', fontsize=10)\n",
    "    axes[idx].set_ylabel(feature, fontsize=10)\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/07_numerical_vs_target.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked bar charts for categorical features\n",
    "fig, axes = plt.subplots(4, 2, figsize=(15, 16))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    ct = pd.crosstab(df[feature], df['target'], normalize='index') * 100\n",
    "    \n",
    "    ct.plot(kind='bar', stacked=True, ax=axes[idx], color=['#66c2a5', '#fc8d62'], \n",
    "            edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    axes[idx].set_title(f'{feature} Distribution by Target', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature, fontsize=10)\n",
    "    axes[idx].set_ylabel('Percentage', fontsize=10)\n",
    "    axes[idx].legend(['No Disease', 'Disease'], loc='best')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    axes[idx].set_xticklabels(axes[idx].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/08_categorical_vs_target.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using IQR method\n",
    "def detect_outliers_iqr(df, feature):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "print(\"Outlier Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "outlier_summary = []\n",
    "for feature in numerical_features:\n",
    "    n_outliers, lower, upper = detect_outliers_iqr(df, feature)\n",
    "    outlier_summary.append({\n",
    "        'Feature': feature,\n",
    "        'Outliers': n_outliers,\n",
    "        'Percentage': f\"{(n_outliers/len(df))*100:.2f}%\",\n",
    "        'Lower_Bound': f\"{lower:.2f}\",\n",
    "        'Upper_Bound': f\"{upper:.2f}\"\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for all numerical features\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Normalize data for better visualization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_normalized = pd.DataFrame(\n",
    "    scaler.fit_transform(df[numerical_features]),\n",
    "    columns=numerical_features\n",
    ")\n",
    "\n",
    "bp = ax.boxplot([df_normalized[col].dropna() for col in numerical_features],\n",
    "                 labels=numerical_features,\n",
    "                 patch_artist=True,\n",
    "                 showmeans=True)\n",
    "\n",
    "# Customize box plot\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('lightblue')\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_title('Outlier Detection - Normalized Features', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Standardized Values', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/09_outliers.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pair Plot Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pair plot for key numerical features\n",
    "selected_features = ['age', 'trestbps', 'chol', 'thalach', 'target']\n",
    "pair_plot = sns.pairplot(df[selected_features], hue='target', \n",
    "                         palette={0: '#66c2a5', 1: '#fc8d62'},\n",
    "                         diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "\n",
    "pair_plot.fig.suptitle('Pair Plot - Key Numerical Features', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.savefig('../screenshots/10_pairplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-tests for numerical features\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "print(\"Statistical Significance Tests (T-test):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stat_results = []\n",
    "for feature in numerical_features:\n",
    "    group0 = df[df['target'] == 0][feature].dropna()\n",
    "    group1 = df[df['target'] == 1][feature].dropna()\n",
    "    \n",
    "    t_stat, p_value = ttest_ind(group0, group1)\n",
    "    \n",
    "    stat_results.append({\n",
    "        'Feature': feature,\n",
    "        'T-Statistic': f\"{t_stat:.4f}\",\n",
    "        'P-Value': f\"{p_value:.4f}\",\n",
    "        'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "    })\n",
    "\n",
    "stat_df = pd.DataFrame(stat_results)\n",
    "print(stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Quality Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATASET SIZE:\")\n",
    "print(f\"   - Total Samples: {len(df)}\")\n",
    "print(f\"   - Features: {len(df.columns) - 1}\")\n",
    "print(f\"   - Numerical: {len(numerical_features)}\")\n",
    "print(f\"   - Categorical: {len(categorical_features)}\")\n",
    "\n",
    "print(f\"\\n2. MISSING VALUES:\")\n",
    "total_missing = df.isnull().sum().sum()\n",
    "print(f\"   - Total: {total_missing} ({(total_missing/(len(df)*len(df.columns)))*100:.2f}%)\")\n",
    "if total_missing > 0:\n",
    "    print(f\"   - Features affected: {missing_df[missing_df['Missing_Count'] > 0].index.tolist()}\")\n",
    "\n",
    "print(f\"\\n3. CLASS BALANCE:\")\n",
    "print(f\"   - Class 0: {target_counts[0]} ({target_percentages[0]:.2f}%)\")\n",
    "print(f\"   - Class 1: {target_counts[1]} ({target_percentages[1]:.2f}%)\")\n",
    "print(f\"   - Balance Status: {'Balanced' if 0.4 <= target_percentages[1]/100 <= 0.6 else 'Imbalanced'}\")\n",
    "\n",
    "print(f\"\\n4. TOP CORRELATIONS WITH TARGET:\")\n",
    "top_corr = target_corr.drop('target').abs().sort_values(ascending=False).head(5)\n",
    "for feat, corr_val in top_corr.items():\n",
    "    print(f\"   - {feat}: {target_corr[feat]:.3f}\")\n",
    "\n",
    "print(f\"\\n5. OUTLIERS DETECTED:\")\n",
    "total_outliers = outlier_df['Outliers'].sum()\n",
    "print(f\"   - Total outliers: {total_outliers}\")\n",
    "if total_outliers > 0:\n",
    "    for _, row in outlier_df.iterrows():\n",
    "        if row['Outliers'] > 0:\n",
    "            print(f\"   - {row['Feature']}: {row['Outliers']} ({row['Percentage']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS FOR MODEL DEVELOPMENT:\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "1. Dataset is relatively small (303 samples) - consider cross-validation\n",
    "2. Class balance is good - no need for SMOTE/oversampling\n",
    "3. Some missing values in 'ca' and 'thal' - need imputation strategy\n",
    "4. Strong predictors identified: cp, thalach, oldpeak, exang\n",
    "5. Some outliers present but may be clinically valid - handle carefully\n",
    "6. Features have different scales - standardization required\n",
    "7. Some categorical features need encoding\n",
    "8. No severe multicollinearity detected\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values before exporting\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Impute missing values with median for numerical features\n",
    "for col in df_clean.columns:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        if col in numerical_features:\n",
    "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
    "        else:\n",
    "            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv('../data/heart_disease_clean.csv', index=False)\n",
    "print(\"Clean dataset saved to: ../data/heart_disease_clean.csv\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n",
    "print(f\"Missing values: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This EDA has revealed:\n",
    "- A well-balanced binary classification problem\n",
    "- 303 patient records with 13 features\n",
    "- Minimal missing data (< 2%)\n",
    "- Several strong predictive features (cp, thalach, oldpeak)\n",
    "- Need for feature scaling and encoding\n",
    "- Dataset ready for model development\n",
    "\n",
    "**Next Steps:**\n",
    "1. Feature engineering and preprocessing\n",
    "2. Model selection and training\n",
    "3. Hyperparameter tuning\n",
    "4. Model evaluation and validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
