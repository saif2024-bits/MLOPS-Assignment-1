{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Experiment Tracking - Heart Disease Prediction\n",
    "## MLOps Assignment - Task 3\n",
    "\n",
    "**Objective:** Track ML experiments using MLflow to compare models, log parameters, metrics, and artifacts\n",
    "\n",
    "**Key Features:**\n",
    "- Automated experiment tracking\n",
    "- Parameter logging for reproducibility\n",
    "- Metric tracking across multiple runs\n",
    "- Artifact logging (models, plots, data)\n",
    "- Experiment comparison and visualization\n",
    "\n",
    "**Models Tracked:**\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "from preprocessing import load_data, create_preprocessing_pipeline\n",
    "from train_mlflow import MLflowModelTrainer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLflow Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name\n",
    "experiment_name = \"heart-disease-prediction\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get experiment info\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "print(f\"Experiment Name: {experiment_name}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "print(f\"\\nMLflow Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "X, y = load_data('../data/heart_disease_clean.csv')\n",
    "\n",
    "print(f\"\\nDataset Shape: {X.shape}\")\n",
    "print(f\"Target Distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "print(\"\\nCreating preprocessing pipeline...\")\n",
    "preprocessing_pipeline = create_preprocessing_pipeline(\n",
    "    handle_outliers=True,\n",
    "    feature_engineering=True\n",
    ")\n",
    "\n",
    "# Transform data\n",
    "X_transformed = preprocessing_pipeline.fit_transform(X)\n",
    "print(f\"Transformed Shape: {X_transformed.shape}\")\n",
    "print(f\"Features created: {X_transformed.shape[1] - X.shape[1]}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment 1 - Logistic Regression with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 1: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Logistic_Regression_Baseline\") as run:\n",
    "    \n",
    "    # Set tags\n",
    "    mlflow.set_tag(\"model_type\", \"Logistic Regression\")\n",
    "    mlflow.set_tag(\"framework\", \"sklearn\")\n",
    "    mlflow.set_tag(\"purpose\", \"baseline_model\")\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'max_iter': 1000,\n",
    "        'solver': 'liblinear',\n",
    "        'C': 1.0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    for param, value in params.items():\n",
    "        mlflow.log_param(param, value)\n",
    "    \n",
    "    mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "    mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric, value in metrics.items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Create and log confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "               xticklabels=['No Disease', 'Disease'],\n",
    "               yticklabels=['No Disease', 'Disease'])\n",
    "    ax.set_title('Logistic Regression - Confusion Matrix', fontweight='bold')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "    \n",
    "    cm_path = '../screenshots/lr_confusion_matrix_mlflow.png'\n",
    "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nRun ID: {run.info.run_id}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    lr_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment 2 - Random Forest with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 2: RANDOM FOREST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Random_Forest_Ensemble\") as run:\n",
    "    \n",
    "    # Set tags\n",
    "    mlflow.set_tag(\"model_type\", \"Random Forest\")\n",
    "    mlflow.set_tag(\"framework\", \"sklearn\")\n",
    "    mlflow.set_tag(\"purpose\", \"ensemble_model\")\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 5,\n",
    "        'min_samples_leaf': 2,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    for param, value in params.items():\n",
    "        mlflow.log_param(param, value)\n",
    "    \n",
    "    mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "    mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric, value in metrics.items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # Create and log feature importance\n",
    "    importances = model.feature_importances_\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sorted_idx = np.argsort(importances)[-15:]\n",
    "    ax.barh(range(len(sorted_idx)), importances[sorted_idx],\n",
    "           color='coral', alpha=0.8, edgecolor='black')\n",
    "    ax.set_yticks(range(len(sorted_idx)))\n",
    "    ax.set_yticklabels([f'Feature {i}' for i in sorted_idx])\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Random Forest - Feature Importance (Top 15)', fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    fi_path = '../screenshots/rf_feature_importance_mlflow.png'\n",
    "    plt.savefig(fi_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(fi_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nRun ID: {run.info.run_id}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    rf_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment 3 - XGBoost with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 3: XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "with mlflow.start_run(run_name=\"XGBoost_Production\") as run:\n",
    "    \n",
    "    # Set tags\n",
    "    mlflow.set_tag(\"model_type\", \"XGBoost\")\n",
    "    mlflow.set_tag(\"framework\", \"xgboost\")\n",
    "    mlflow.set_tag(\"purpose\", \"production_candidate\")\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 5,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    \n",
    "    # Log parameters\n",
    "    for param, value in params.items():\n",
    "        mlflow.log_param(param, value)\n",
    "    \n",
    "    mlflow.log_param(\"train_samples\", X_train.shape[0])\n",
    "    mlflow.log_param(\"test_samples\", X_test.shape[0])\n",
    "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
    "    \n",
    "    # Train model\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    # Log metrics\n",
    "    for metric, value in metrics.items():\n",
    "        mlflow.log_metric(metric, value)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.xgboost.log_model(model, \"model\")\n",
    "    \n",
    "    # Create and log ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, color='green', linewidth=2,\n",
    "           label=f'XGBoost (AUC = {metrics[\"roc_auc\"]:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('XGBoost - ROC Curve', fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    roc_path = '../screenshots/xgb_roc_curve_mlflow.png'\n",
    "    plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(roc_path)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nRun ID: {run.info.run_id}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    xgb_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare All Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all runs from the experiment\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = mlflow.get_experiment_by_name(\"heart-disease-prediction\")\n",
    "runs = client.search_runs(experiment.experiment_id)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for run in runs:\n",
    "    comparison_data.append({\n",
    "        'Run Name': run.data.tags.get('mlflow.runName', 'Unknown'),\n",
    "        'Model Type': run.data.tags.get('model_type', 'Unknown'),\n",
    "        'Accuracy': run.data.metrics.get('accuracy', 0),\n",
    "        'Precision': run.data.metrics.get('precision', 0),\n",
    "        'Recall': run.data.metrics.get('recall', 0),\n",
    "        'F1 Score': run.data.metrics.get('f1_score', 0),\n",
    "        'ROC-AUC': run.data.metrics.get('roc_auc', 0),\n",
    "        'Run ID': run.info.run_id\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('ROC-AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Experiment Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n",
    "colors = ['steelblue', 'coral', 'lightgreen']\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    data = comparison_df.sort_values('Model Type')\n",
    "    axes[idx].bar(range(len(data)), data[metric], \n",
    "                  color=colors[:len(data)], alpha=0.8, edgecolor='black')\n",
    "    axes[idx].set_xticks(range(len(data)))\n",
    "    axes[idx].set_xticklabels(data['Model Type'], rotation=45, ha='right')\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    axes[idx].set_title(f'{metric} Comparison', fontweight='bold')\n",
    "    axes[idx].set_ylim([0, 1.1])\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate(data[metric]):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Summary table in last subplot\n",
    "axes[5].axis('off')\n",
    "table_data = comparison_df[['Model Type', 'ROC-AUC', 'Accuracy', 'F1 Score']].values\n",
    "table = axes[5].table(cellText=table_data,\n",
    "                      colLabels=['Model', 'ROC-AUC', 'Accuracy', 'F1'],\n",
    "                      cellLoc='center',\n",
    "                      loc='center',\n",
    "                      bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "axes[5].set_title('Summary Table', fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../screenshots/mlflow_all_experiments_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comparison plot saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Best Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run\n",
    "best_run = comparison_df.iloc[0]\n",
    "best_run_id = best_run['Run ID']\n",
    "best_model_type = best_run['Model Type']\n",
    "\n",
    "print(f\"Best Model: {best_model_type}\")\n",
    "print(f\"Best ROC-AUC: {best_run['ROC-AUC']:.4f}\")\n",
    "print(f\"Run ID: {best_run_id}\")\n",
    "\n",
    "# Load model from MLflow\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "\n",
    "if best_model_type == \"XGBoost\":\n",
    "    loaded_model = mlflow.xgboost.load_model(model_uri)\n",
    "else:\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"\\nModel loaded successfully from MLflow!\")\n",
    "print(f\"Model type: {type(loaded_model).__name__}\")\n",
    "\n",
    "# Test prediction\n",
    "test_prediction = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nSample predictions: {test_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MLflow UI Access Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MLFLOW UI ACCESS INSTRUCTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n1. Open a terminal and navigate to the project directory:\")\n",
    "print(\"   cd /Users/saif.afzal/Documents/M.Tech/MLOPS/heart-disease-mlops\")\n",
    "print(\"\\n2. Start MLflow UI:\")\n",
    "print(\"   mlflow ui\")\n",
    "print(\"\\n3. Open your browser and go to:\")\n",
    "print(\"   http://127.0.0.1:5000\")\n",
    "print(\"\\n4. In the MLflow UI, you can:\")\n",
    "print(\"   - View all experiment runs\")\n",
    "print(\"   - Compare models side-by-side\")\n",
    "print(\"   - View logged parameters and metrics\")\n",
    "print(\"   - Download artifacts (models, plots)\")\n",
    "print(\"   - Filter and sort runs\")\n",
    "print(\"   - Create charts and visualizations\")\n",
    "print(\"\\n5. Alternative: Use custom port\")\n",
    "print(\"   mlflow ui --port 8080\")\n",
    "print(\"   Then visit: http://127.0.0.1:8080\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Show experiment location\n",
    "experiment = mlflow.get_experiment_by_name(\"heart-disease-prediction\")\n",
    "print(f\"\\nExperiment artifacts stored at:\")\n",
    "print(f\"{experiment.artifact_location}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MLFLOW EXPERIMENT TRACKING - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. EXPERIMENTS TRACKED:\")\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"   - {row['Model Type']}: ROC-AUC = {row['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(\"\\n2. BEST MODEL:\")\n",
    "print(f\"   Model: {best_run['Model Type']}\")\n",
    "print(f\"   ROC-AUC: {best_run['ROC-AUC']:.4f}\")\n",
    "print(f\"   Accuracy: {best_run['Accuracy']:.4f}\")\n",
    "print(f\"   F1 Score: {best_run['F1 Score']:.4f}\")\n",
    "\n",
    "print(\"\\n3. LOGGED ARTIFACTS:\")\n",
    "print(\"   - Trained models (3)\")\n",
    "print(\"   - Confusion matrices\")\n",
    "print(\"   - Feature importance plots\")\n",
    "print(\"   - ROC curves\")\n",
    "print(\"   - Parameter configurations\")\n",
    "print(\"   - All metrics\")\n",
    "\n",
    "print(\"\\n4. KEY CAPABILITIES DEMONSTRATED:\")\n",
    "print(\"   ✓ Automated experiment tracking\")\n",
    "print(\"   ✓ Parameter logging for reproducibility\")\n",
    "print(\"   ✓ Metric tracking across runs\")\n",
    "print(\"   ✓ Artifact management (models, plots)\")\n",
    "print(\"   ✓ Model versioning and registry\")\n",
    "print(\"   ✓ Experiment comparison\")\n",
    "print(\"   ✓ Model loading from runs\")\n",
    "\n",
    "print(\"\\n5. BENEFITS FOR PRODUCTION:\")\n",
    "print(\"   - Reproducible experiments\")\n",
    "print(\"   - Easy model comparison\")\n",
    "print(\"   - Centralized model storage\")\n",
    "print(\"   - Experiment lineage tracking\")\n",
    "print(\"   - Collaboration enabled\")\n",
    "\n",
    "print(\"\\n6. NEXT STEPS:\")\n",
    "print(\"   - Register best model in MLflow Model Registry\")\n",
    "print(\"   - Transition model to Production stage\")\n",
    "print(\"   - Set up model serving with MLflow\")\n",
    "print(\"   - Implement A/B testing framework\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 3 COMPLETE - MLFLOW EXPERIMENT TRACKING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **MLflow Integration**: Successfully integrated MLflow tracking into the training pipeline\n",
    "2. **Experiment Tracking**: Tracked 3 models with all parameters, metrics, and artifacts\n",
    "3. **Reproducibility**: All experiments are reproducible with logged parameters\n",
    "4. **Comparison**: Created comprehensive comparisons of all experiments\n",
    "5. **Artifact Management**: Logged models, plots, and results for each run\n",
    "6. **Model Loading**: Demonstrated loading models from MLflow runs\n",
    "\n",
    "### MLflow Features Used\n",
    "\n",
    "- **Experiments**: Organized runs under named experiment\n",
    "- **Runs**: Each model training as separate run\n",
    "- **Parameters**: All hyperparameters logged\n",
    "- **Metrics**: Performance metrics tracked\n",
    "- **Artifacts**: Models and visualizations saved\n",
    "- **Tags**: Metadata for organization\n",
    "- **Model Registry**: Models logged for versioning\n",
    "\n",
    "### Production-Ready Features\n",
    "\n",
    "- Centralized experiment tracking\n",
    "- Model versioning and lineage\n",
    "- Easy model comparison and selection\n",
    "- Reproducible training runs\n",
    "- Artifact storage and retrieval\n",
    "- Web UI for visualization\n",
    "\n",
    "**Status**: Task 3 Complete ✅"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
