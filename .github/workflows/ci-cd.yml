name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install linting dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pylint black isort

    - name: Run Black (code formatter check)
      run: |
        black --check --diff src/ tests/
      continue-on-error: true

    - name: Run isort (import sorting check)
      run: |
        isort --check-only --diff src/ tests/
      continue-on-error: true

    - name: Run Flake8 (PEP 8 linting)
      run: |
        # Stop the build if there are Python syntax errors or undefined names
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

    - name: Run Pylint (advanced linting)
      run: |
        pylint src/ --exit-zero --rcfile=.pylintrc || true
      continue-on-error: true

  test:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_production.txt
        pip install pytest pytest-cov pytest-xdist

    - name: Download dataset
      run: |
        python data/download_data.py

    - name: Run tests with coverage
      run: |
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
      continue-on-error: true

    - name: Archive coverage reports
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: htmlcov/
        retention-days: 30

  train-model:
    name: Model Training & Validation
    runs-on: ubuntu-latest
    needs: test

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_production.txt

    - name: Download dataset
      run: |
        python data/download_data.py

    - name: Train models
      run: |
        python src/train.py

    - name: Validate model performance
      run: |
        python -c "
        import json

        # Load results
        with open('models/training_results.json', 'r') as f:
            results = json.load(f)

        # Check minimum performance thresholds
        for model_name, metrics in results['results'].items():
            accuracy = metrics['test_accuracy']
            roc_auc = metrics['test_roc_auc']

            print(f'{model_name}:')
            print(f'  - Test Accuracy: {accuracy:.4f}')
            print(f'  - Test ROC-AUC: {roc_auc:.4f}')

            # Validate thresholds
            assert accuracy > 0.6, f'{model_name} accuracy too low: {accuracy}'
            assert roc_auc > 0.7, f'{model_name} ROC-AUC too low: {roc_auc}'

        print('✅ All models meet performance thresholds')
        "

    - name: Upload model artifacts
      uses: actions/upload-artifact@v4
      with:
        name: trained-models
        path: |
          models/*.pkl
          models/*.json
          models/*.md
        retention-days: 90

    - name: Upload training logs
      uses: actions/upload-artifact@v4
      with:
        name: training-logs
        path: |
          screenshots/
        retention-days: 30

  mlflow-tracking:
    name: MLflow Experiment Tracking
    runs-on: ubuntu-latest
    needs: test

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_production.txt

    - name: Download dataset
      run: |
        python data/download_data.py

    - name: Run MLflow experiments
      run: |
        python src/train_mlflow.py

    - name: Upload MLflow artifacts
      uses: actions/upload-artifact@v4
      with:
        name: mlflow-runs
        path: mlruns/
        retention-days: 90

  integration-test:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [train-model, mlflow-tracking]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_production.txt

    - name: Download trained models
      uses: actions/download-artifact@v4
      with:
        name: trained-models
        path: models/

    - name: Download dataset
      run: |
        python data/download_data.py

    - name: Download preprocessing pipeline
      run: |
        python -c "
        from src.preprocessing import load_data, create_preprocessing_pipeline
        import pickle

        # Create and save pipeline
        X, y = load_data('data/heart_disease_clean.csv')
        pipeline = create_preprocessing_pipeline(
            handle_outliers=True,
            feature_engineering=True
        )
        pipeline.fit(X)

        with open('models/preprocessing_pipeline.pkl', 'wb') as f:
            pickle.dump(pipeline, f)
        "

    - name: Test model loading and prediction
      run: |
        python -c "
        from src.model_pipeline import HeartDiseasePredictor

        # Test predictor
        predictor = HeartDiseasePredictor(model_dir='models/')
        predictor.load_models(model_name='xgboost')

        # Test prediction
        patient = {
            'age': 63, 'sex': 1, 'cp': 1, 'trestbps': 145,
            'chol': 233, 'fbs': 1, 'restecg': 2, 'thalach': 150,
            'exang': 0, 'oldpeak': 2.3, 'slope': 3, 'ca': 0, 'thal': 6
        }

        result = predictor.predict(patient)

        assert 'prediction' in result
        assert 'probability' in result
        assert result['prediction'] in [0, 1]

        print(f'✅ Prediction test passed: {result}')
        "

  security-scan:
    name: Security Vulnerability Scan
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install safety
      run: |
        pip install safety

    - name: Run security scan
      run: |
        safety check --json || true
      continue-on-error: true

  notify:
    name: Notification
    runs-on: ubuntu-latest
    needs: [lint, test, train-model, mlflow-tracking, integration-test]
    if: always()

    steps:
    - name: Pipeline Status
      run: |
        echo "==================================="
        echo "CI/CD Pipeline Execution Summary"
        echo "==================================="
        echo "Lint: ${{ needs.lint.result }}"
        echo "Test: ${{ needs.test.result }}"
        echo "Train Model: ${{ needs.train-model.result }}"
        echo "MLflow Tracking: ${{ needs.mlflow-tracking.result }}"
        echo "Integration Test: ${{ needs.integration-test.result }}"
        echo "==================================="

        if [[ "${{ needs.lint.result }}" == "success" ]] && \
           [[ "${{ needs.test.result }}" == "success" ]] && \
           [[ "${{ needs.train-model.result }}" == "success" ]] && \
           [[ "${{ needs.mlflow-tracking.result }}" == "success" ]] && \
           [[ "${{ needs.integration-test.result }}" == "success" ]]; then
          echo "✅ All pipeline stages passed!"
          exit 0
        else
          echo "❌ Some pipeline stages failed"
          exit 1
        fi
